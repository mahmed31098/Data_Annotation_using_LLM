{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "75e6c6df-43f0-41f0-fb6d-0dd693a0e95e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmpahmed/miniforge3/envs/nli_jupyter/lib/python3.10/site-packages/unsloth/__init__.py:40: UserWarning: Unsloth: 'CUDA_VISIBLE_DEVICES' is currently 0,1 \n",
      "Unsloth currently does not work on multi GPU setups - sadly we are a 2 brother team so enabling it will require much more work, so we have to prioritize. Please understand!We do have a beta version, which you can contact us about!\n",
      "Thank you for your understanding and we appreciate it immensely!\n",
      "\n",
      "Multiple CUDA devices detected but we require a single device.\n",
      "We will override CUDA_VISIBLE_DEVICES to first device: 0.\n",
      "  warnings.warn(\n",
      "/home/tmpahmed/miniforge3/envs/nli_jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/codellama-34b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "a96daf96-9ea7-4095-9b52-342ef7971db0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "\n",
    "If you want to use the `ChatML` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing).\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SjYnuy12Dw1_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'training.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Map sentiment labels\n",
    "label_mapping = {'NEG': 'Negative', 'NEU': 'Neutral', 'POS': 'Positive'}\n",
    "df['SENTIMENT'] = df['SENTIMENT'].map(label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.array(df['TEXT'].tolist())\n",
    "y = np.array(df['SENTIMENT'].tolist())\n",
    "\n",
    "# Prepare the data dictionary\n",
    "data_dict = {\n",
    "    'X_train': X,\n",
    "    'y_train': y,\n",
    "}\n",
    "\n",
    "formatted_dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bcba961b9c8240a4bbaf9ea56141cf56",
      "7a1415ca56ee421fb3628ae74b3fd000",
      "c0971cb118a749e4b2e6625f168ee15b",
      "0d85969d7f744cec97dc66df4d564939",
      "e786c3f4e5ee4a8bbae05de14f201532",
      "a8f2f72102be48bab5b2d8a38a1fff78",
      "49443d7bb4a7428284d522133e4f8660",
      "e881687e65094052b9eedb1a91ec54db",
      "4929f10d74cb413e9a84a9cce79b1773",
      "ca06a8c66197469f888ecd0657896f20",
      "d5bf5453d1f64fd2b960d08685cc1399"
     ]
    },
    "id": "Lzgzm-R1entO",
    "outputId": "48e8a422-9a54-46b3-86db-340a14468166"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1132/1132 [00:00<00:00, 145911.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Based on the following French text content, identify the sentiment of the text from sentiments_list: ['Positive', 'Neutral', 'Negative']\n",
    "If the text does not fit into any of the above mentioned sentiments, please return the answer nearest to any of the sentiments mentioned in sentiments_list.\n",
    "Return only one sentiment from the sentiments_list above.\n",
    "\n",
    "Examples:\n",
    "Positive:\n",
    "- \"J'adore cette nouvelle fonctionnalit√©! Elle est vraiment incroyable et facilite tellement les choses.\"\n",
    "- \"Quel excellent service! Je suis vraiment satisfait de mon exp√©rience.\"\n",
    "\n",
    "Neutral:\n",
    "- \"Le produit fonctionne comme pr√©vu.\"\n",
    "- \"Les instructions √©taient claires et pr√©cises.\"\n",
    "\n",
    "Negative:\n",
    "- \"Je suis tr√®s d√©√ßu par la qualit√© de ce produit. Il ne fonctionne pas correctement.\"\n",
    "- \"Le service client √©tait tr√®s mauvais et n'a pas r√©solu mon probl√®me.\"\n",
    "### text:\n",
    "{}\n",
    "\n",
    "### category:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    text = examples[\"X_train\"]\n",
    "    category     = examples[\"y_train\"]\n",
    "    #categories_list = examples[\"categories_list\"]\n",
    "    contents = []\n",
    "    for X_train, y_train in zip(text, category):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        content = prompt.format(X_train, y_train) + EOS_TOKEN\n",
    "        contents.append(content)\n",
    "    return { \"response\" : contents, }\n",
    "pass\n",
    "\n",
    "dataset = formatted_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-z1onb5Dw2D",
    "outputId": "ac10b32d-f039-4fe2-d3f5-88658c05bd5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['X_train', 'y_train', 'response'],\n",
       "    num_rows: 1132\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "a44fe9ea08fa4a308c15919f9f20582d",
      "2aa1cf9ab4de4019b37dfe9ce9af7eb4",
      "4ee5f9b6dca4477c87e5630b73171684",
      "31b8c4e3f4ae4feabcc06abb37dadb66",
      "72781c0a14374f0b8e9bbc064e50c11d",
      "4b406f455c39409a8f841d4bb90805fc",
      "76ad6d2dbb50456791915c65ece277b1",
      "068e2427cd6c4c84b74376d96be0365f",
      "2ebd3a50e23e40d5b885da38a2bf255a",
      "03e837f34f624f75b8926425780d2bca",
      "ff11d0746a904f3590c5e3a8329f2eb3"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "16f63b41-4e82-4f82-944b-1963e4595a9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmpahmed/miniforge3/envs/nli_jupyter/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/tmpahmed/miniforge3/envs/nli_jupyter/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/tmpahmed/miniforge3/envs/nli_jupyter/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/tmpahmed/miniforge3/envs/nli_jupyter/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1132/1132 [00:00<00:00, 2604.41 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"response\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # Set num_train_epochs = 1 for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "a8b71e8c-5eb3-462c-c297-341371c04a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-PCIE-40GB. Max memory = 39.394 GB.\n",
      "4.5 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "06f601d1-cbb0-40db-ce61-4b7dd101b2cb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,132 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 108,920,832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.620300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "6299fdff-2867-4943-9b9e-cbb0fd28a545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.2305 seconds used for training.\n",
      "1.4 minutes used for training.\n",
      "Peak reserved memory = 5.072 GB.\n",
      "Peak reserved memory for training = 0.572 GB.\n",
      "Peak reserved memory % of max memory = 12.875 %.\n",
      "Peak reserved memory for training % of max memory = 1.452 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wLj661kGfQ9A"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"h_anno_codellama34B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RrGWtCwAQ71N",
    "outputId": "8843bad7-9e22-466b-e74c-969a63946a17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('h_anno_codellama34B/tokenizer_config.json',\n",
       " 'h_anno_codellama34B/special_tokens_map.json',\n",
       " 'h_anno_codellama34B/tokenizer.model',\n",
       " 'h_anno_codellama34B/added_tokens.json',\n",
       " 'h_anno_codellama34B/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"h_anno_codellama34B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model using one instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbHgHmWLQ71N",
    "outputId": "091795d9-2117-47bf-fbf1-1a801c8767fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s> Based on the following French text content, identify the sentiment of the text from sentiments_list: [\\'Positive\\', \\'Neutral\\', \\'Negative\\']\\nIf the text does not fit into any of the above mentioned sentiments, please return the answer nearest to any of the sentiments mentioned in sentiments_list.\\nReturn only one sentiment from the sentiments_list above.\\n\\nExamples:\\nPositive:\\n- \"J\\'adore cette nouvelle fonctionnalit√©! Elle est vraiment incroyable et facilite tellement les choses.\"\\n- \"Quel excellent service! Je suis vraiment satisfait de mon exp√©rience.\"\\n\\nNeutral:\\n- \"Le produit fonctionne comme pr√©vu.\"\\n- \"Les instructions √©taient claires et pr√©cises.\"\\n\\nNegative:\\n- \"Je suis tr√®s d√©√ßu par la qualit√© de ce produit. Il ne fonctionne pas correctement.\"\\n- \"Le service client √©tait tr√®s mauvais et n\\'a pas r√©solu mon probl√®me.\"\\n### text:\\nTu es belle\\n\\n### category:\\n\\n### sentiment:\\nPositive\\n\\n### sentiment_confidence_scores:\\n{\\'Positive\\': 0.9999999403953552, \\'Neutral\\': 1.496300108032']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/codellama-34b-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "prompt = \"\"\"Based on the following French text content, identify the sentiment of the text from sentiments_list: ['Positive', 'Neutral', 'Negative']\n",
    "If the text does not fit into any of the above mentioned sentiments, please return the answer nearest to any of the sentiments mentioned in sentiments_list.\n",
    "Return only one sentiment from the sentiments_list above.\n",
    "\n",
    "Examples:\n",
    "Positive:\n",
    "- \"J'adore cette nouvelle fonctionnalit√©! Elle est vraiment incroyable et facilite tellement les choses.\"\n",
    "- \"Quel excellent service! Je suis vraiment satisfait de mon exp√©rience.\"\n",
    "\n",
    "Neutral:\n",
    "- \"Le produit fonctionne comme pr√©vu.\"\n",
    "- \"Les instructions √©taient claires et pr√©cises.\"\n",
    "\n",
    "Negative:\n",
    "- \"Je suis tr√®s d√©√ßu par la qualit√© de ce produit. Il ne fonctionne pas correctement.\"\n",
    "- \"Le service client √©tait tr√®s mauvais et n'a pas r√©solu mon probl√®me.\"\n",
    "### text:\n",
    "{}\n",
    "\n",
    "### category:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        \"Tu es belle\", # text\n",
    "        \"\", # category\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By running the following code we will get dataset['training_result'] containing the sentiment for each text in dataset['X_train'] using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PV8nqCO2Q71N",
    "outputId": "db9b9a8f-c11a-41ea-ada3-1a485ac7a947",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import torch\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"h_anno_codellama34B\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "training_results = [] # Create an empty list to store the results\n",
    "\n",
    "for text in dataset['X_train']: # Iterate through each text in dataset['X_train']\n",
    "    prompt_text = prompt.format(text, \"\") # Format the prompt with the current text\n",
    "    inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\") # Tokenize the input\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True) # Generate the output\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True) # Decode the output and extract the sentiment\n",
    "    sentiment = decoded_output[0].strip()\n",
    "    training_results.append(sentiment) # Append the sentiment to the results list\n",
    "\n",
    "dataset = dataset.add_column('training_result', training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUGiiG3ZhHlF",
    "outputId": "0d8c09eb-5194-476c-9a59-f0c9f6d689c0"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "l-g2uqY9hHn9",
    "outputId": "e9396722-3138-4190-e2b6-bb40bd84d1f2"
   },
   "outputs": [],
   "source": [
    "dataset['training_result'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "e6vhcYPSj2aR",
    "outputId": "898c3f12-c8e0-4755-94de-0a7aa8d60258"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the following French text content, identify the sentiment of the text from sentiments_list: [\\'Positive\\', \\'Neutral\\', \\'Negative\\']\\nIf the text does not fit into any of the above mentioned sentiments, please return the answer nearest to any of the sentiments mentioned in sentiments_list.\\nReturn only one sentiment from the sentiments_list above.\\n\\nExamples:\\nPositive:\\n- \"J\\'adore cette nouvelle fonctionnalit√©! Elle est vraiment incroyable et facilite tellement les choses.\"\\n- \"Quel excellent service! Je suis vraiment satisfait de mon exp√©rience.\"\\n\\nNeutral:\\n- \"Le produit fonctionne comme pr√©vu.\"\\n- \"Les instructions √©taient claires et pr√©cises.\"\\n\\nNegative:\\n- \"Je suis tr√®s d√©√ßu par la qualit√© de ce produit. Il ne fonctionne pas correctement.\"\\n- \"Le service client √©tait tr√®s mauvais et n\\'a pas r√©solu mon probl√®me.\"\\n### text:\\nJe voudrais qu\\'on arr√™te de se moquer de moi et ma culture\\n\\n### category:\\n\\nNeutral'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['training_result'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "0Ro7nMnckhny",
    "outputId": "1f3a6452-6a66-4593-f37f-ef8cce0eba1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the following French text content, identify the sentiment of the text from sentiments_list: [\\'Positive\\', \\'Neutral\\', \\'Negative\\']\\nIf the text does not fit into any of the above mentioned sentiments, please return the answer nearest to any of the sentiments mentioned in sentiments_list.\\nReturn only one sentiment from the sentiments_list above.\\n\\nExamples:\\nPositive:\\n- \"J\\'adore cette nouvelle fonctionnalit√©! Elle est vraiment incroyable et facilite tellement les choses.\"\\n- \"Quel excellent service! Je suis vraiment satisfait de mon exp√©rience.\"\\n\\nNeutral:\\n- \"Le produit fonctionne comme pr√©vu.\"\\n- \"Les instructions √©taient claires et pr√©cises.\"\\n\\nNegative:\\n- \"Je suis tr√®s d√©√ßu par la qualit√© de ce produit. Il ne fonctionne pas correctement.\"\\n- \"Le service client √©tait tr√®s mauvais et n\\'a pas r√©solu mon probl√®me.\"\\n### text:\\nSalut Julie c\\'est avec toi que je voulais parler\\n\\n### category:\\n\\nNeutral'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['training_result'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to save early\n",
    "#dataset.to_excel('unfinetuned_Mistral.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HHHqADJWhHqd"
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(dataset)\n",
    "substring_to_remove = 'Based on the following French text content, identify the sentiment of the text from sentiments_list: [\\'Positive\\', \\'Neutral\\', \\'Negative\\']\\nIf the text does not fit into any of the above mentioned sentiments, please return the answer nearest to any of the sentiments mentioned in sentiments_list.\\nReturn only one sentiment from the sentiments_list above.\\n\\nExamples:\\nPositive:\\n- \"J\\'adore cette nouvelle fonctionnalit√©! Elle est vraiment incroyable et facilite tellement les choses.\"\\n- \"Quel excellent service! Je suis vraiment satisfait de mon exp√©rience.\"\\n\\nNeutral:\\n- \"Le produit fonctionne comme pr√©vu.\"\\n- \"Les instructions √©taient claires et pr√©cises.\"\\n\\nNegative:\\n- \"Je suis tr√®s d√©√ßu par la qualit√© de ce produit. Il ne fonctionne pas correctement.\"\\n- \"Le service client √©tait tr√®s mauvais et n\\'a pas r√©solu mon probl√®me.\"\\n### text:'\n",
    "\n",
    "df2['training_result'] = df2['training_result'].str.replace(substring_to_remove, '', regex=False) # Remove the substring from the 'training_result' column\n",
    "\n",
    "sentiments_list = ['Negative', 'Neutral', 'Positive'] # Define the list of words to search for\n",
    "\n",
    "def find_and_replace_sentiment(text): # Function to find and replace with the sentiment word\n",
    "    for sentiment in sentiments_list:\n",
    "        if sentiment in text:\n",
    "            return sentiment\n",
    "    return None\n",
    "\n",
    "df2['training_result'] = df2['training_result'].apply(find_and_replace_sentiment) # Apply the function to the 'training_result' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "0x06wcf2hHsx",
    "outputId": "5b6009f6-1cfa-4957-c484-9bccee3e79b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_train</th>\n",
       "      <th>y_train</th>\n",
       "      <th>response</th>\n",
       "      <th>training_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fa√ßon j'veux ps d'une crasseuse comme toi</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claire c'est pas le mot</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je voudrais qu'on arr√™te de se moquer de moi e...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salut Julie c'est avec toi que je voulais parler</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>la vie t'es fat ima</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>tu kleh ou quoi</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>sale feuj</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>juif de mort</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>amine vient 1v1 devant le lyc√©√©</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>vsy demain 15h √† la sortie tu vas voir</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Based on the following French text content, id...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1132 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                X_train   y_train  \\\n",
       "0             fa√ßon j'veux ps d'une crasseuse comme toi  Negative   \n",
       "1                               Claire c'est pas le mot   Neutral   \n",
       "2     Je voudrais qu'on arr√™te de se moquer de moi e...  Positive   \n",
       "3      Salut Julie c'est avec toi que je voulais parler   Neutral   \n",
       "4                                   la vie t'es fat ima  Negative   \n",
       "...                                                 ...       ...   \n",
       "1127                                    tu kleh ou quoi   Neutral   \n",
       "1128                                          sale feuj  Negative   \n",
       "1129                                       juif de mort  Negative   \n",
       "1130                    amine vient 1v1 devant le lyc√©√©  Negative   \n",
       "1131             vsy demain 15h √† la sortie tu vas voir  Negative   \n",
       "\n",
       "                                               response training_result  \n",
       "0     Based on the following French text content, id...        Negative  \n",
       "1     Based on the following French text content, id...         Neutral  \n",
       "2     Based on the following French text content, id...         Neutral  \n",
       "3     Based on the following French text content, id...         Neutral  \n",
       "4     Based on the following French text content, id...        Negative  \n",
       "...                                                 ...             ...  \n",
       "1127  Based on the following French text content, id...        Negative  \n",
       "1128  Based on the following French text content, id...        Negative  \n",
       "1129  Based on the following French text content, id...        Negative  \n",
       "1130  Based on the following French text content, id...         Neutral  \n",
       "1131  Based on the following French text content, id...         Neutral  \n",
       "\n",
       "[1132 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jBGjs5LwhHvI"
   },
   "outputs": [],
   "source": [
    "df2.to_excel('finetuned_Codellama34B.xlsx', index=False) #Save finally"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03e837f34f624f75b8926425780d2bca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "068e2427cd6c4c84b74376d96be0365f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d85969d7f744cec97dc66df4d564939": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca06a8c66197469f888ecd0657896f20",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d5bf5453d1f64fd2b960d08685cc1399",
      "value": "‚Äá1132/1132‚Äá[00:00&lt;00:00,‚Äá26316.98‚Äáexamples/s]"
     }
    },
    "2aa1cf9ab4de4019b37dfe9ce9af7eb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b406f455c39409a8f841d4bb90805fc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_76ad6d2dbb50456791915c65ece277b1",
      "value": "Map‚Äá(num_proc=2):‚Äá100%"
     }
    },
    "2ebd3a50e23e40d5b885da38a2bf255a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "31b8c4e3f4ae4feabcc06abb37dadb66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03e837f34f624f75b8926425780d2bca",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ff11d0746a904f3590c5e3a8329f2eb3",
      "value": "‚Äá6/6‚Äá[00:00&lt;00:00,‚Äá‚Äá9.64‚Äáexamples/s]"
     }
    },
    "4929f10d74cb413e9a84a9cce79b1773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49443d7bb4a7428284d522133e4f8660": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b406f455c39409a8f841d4bb90805fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ee5f9b6dca4477c87e5630b73171684": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_068e2427cd6c4c84b74376d96be0365f",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ebd3a50e23e40d5b885da38a2bf255a",
      "value": 6
     }
    },
    "72781c0a14374f0b8e9bbc064e50c11d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76ad6d2dbb50456791915c65ece277b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a1415ca56ee421fb3628ae74b3fd000": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8f2f72102be48bab5b2d8a38a1fff78",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_49443d7bb4a7428284d522133e4f8660",
      "value": "Map:‚Äá100%"
     }
    },
    "a44fe9ea08fa4a308c15919f9f20582d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2aa1cf9ab4de4019b37dfe9ce9af7eb4",
       "IPY_MODEL_4ee5f9b6dca4477c87e5630b73171684",
       "IPY_MODEL_31b8c4e3f4ae4feabcc06abb37dadb66"
      ],
      "layout": "IPY_MODEL_72781c0a14374f0b8e9bbc064e50c11d"
     }
    },
    "a8f2f72102be48bab5b2d8a38a1fff78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcba961b9c8240a4bbaf9ea56141cf56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a1415ca56ee421fb3628ae74b3fd000",
       "IPY_MODEL_c0971cb118a749e4b2e6625f168ee15b",
       "IPY_MODEL_0d85969d7f744cec97dc66df4d564939"
      ],
      "layout": "IPY_MODEL_e786c3f4e5ee4a8bbae05de14f201532"
     }
    },
    "c0971cb118a749e4b2e6625f168ee15b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e881687e65094052b9eedb1a91ec54db",
      "max": 1132,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4929f10d74cb413e9a84a9cce79b1773",
      "value": 1132
     }
    },
    "ca06a8c66197469f888ecd0657896f20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5bf5453d1f64fd2b960d08685cc1399": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e786c3f4e5ee4a8bbae05de14f201532": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e881687e65094052b9eedb1a91ec54db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff11d0746a904f3590c5e3a8329f2eb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
